# LLM Evaluation

Evaluation metrics for performance and quality of generated text

* Perplexity - measures how well the model predicts the next word in a sequence
* BLEU - score measures the similarity between the generated text and a reference text
* Human evaluation - involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance
