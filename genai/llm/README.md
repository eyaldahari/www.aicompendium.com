# LLM

Glossary

<table><thead><tr><th width="114">Training</th><th></th></tr></thead><tbody><tr><td>epoch</td><td>one go through the entire dataset of training samples</td></tr><tr><td>batch</td><td>each epoch is divided to batches since we can't get all dataset sample in one go due to context window (however, we can parallel the computation if we have multiple GPU's)</td></tr><tr><td>step</td><td>1 batch of training data</td></tr></tbody></table>
